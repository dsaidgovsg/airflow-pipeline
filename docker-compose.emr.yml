version: '2.2'
services:
  postgres:
    hostname: postgres
    image: postgres:9.6
    ports:
      - 127.0.0.1:5999:5432
    restart: always
    volumes:
      - postgres:/var/lib/postgresql/data
    environment:
      POSTGRES_USER: fixme
      POSTGRES_PASSWORD: fixme
      POSTGRES_DB: airflow
  scheduler:
    build:
      context: .
      dockerfile: Dockerfile-runnable
    command: ['afp-scheduler']
    depends_on:
      - postgres
    environment:
      AIRFLOW__CORE__FERNET_KEY: ZZvP_7JEhKfQ7v5sg1DV1o4b1DcGv3Tu9vJXzEKkxbE=
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql://fixme:fixme@localhost:5999/airflow
      # IMPORTANT ENV VARS FOR EMR BELOW
      SPARK_HOME: /usr/lib/spark
      PYTHONPATH: /usr/lib/spark/python:/usr/lib/spark/python/lib/py4j-src.zip
      SPARK_OPTS: "--conf spark.yarn.dist.files=file:/usr/lib/spark/python/lib/pyspark.zip,file:/usr/lib/spark/python/lib/py4j-src.zip
                   --conf spark.executorEnv.PYTHONPATH=pyspark.zip:py4j-src.zip
                  "
      HADOOP_COMMON_LIB_NATIVE_DIR: null
      HADOOP_HOME: null
      HADOOP_CONF_DIR: null
      HADOOP_COMMON_HOME: null
      HADOOP_MAPRED_HOME: null
      HADOOP_YARN_HOME: null
      HADOOP_HTTPFS_HOME: null
      HADOOP_HDFS_HOME: null
      HADOOP_KMS_HOME: null
      HADOOP_OPTS: null
      HDFS_CONF_DIR: null
      HIVE_CONF_DIR: null
      YARN_CONF_DIR: null
      PYSPARK_PYTHON: null
      PYSPARK_SUBMIT_ARGS: null
      SPARK_MASTER_OPTS: null
      SPARK_MASTER_PORT: null
      SPARK_MASTER_WEBUI_PORT: null
      SPARK_PACKAGES: null
      SPARK_WORKER_OPTS: null
      SPARK_WORKER_PORT: null
      SPARK_WORKER_WEBUI_PORT: null
      # IMPORTANT ENV VARS FOR EMR ABOVE, OVERRIDE THEM IF NECESSARY OR ADD ADDITIONAL ENV VARS BELOW
    hostname: scheduler
    network_mode: host
    restart: always
    volumes:
      - airflow_logs:/airflow/logs:rw
      # IMPORTANT BIND MOUNTS FOR EMR BELOW
      - /etc/alternatives/hadoop-conf:/etc/alternatives/hadoop-conf:ro
      - /etc/alternatives/hive-conf:/etc/alternatives/hive-conf:ro
      - /etc/alternatives/spark-conf:/etc/alternatives/spark-conf:ro
      - /etc/spark/conf:/etc/spark/conf:ro
      - /etc/hadoop/conf:/etc/hadoop/conf:ro
      - /etc/hive/conf:/etc/hive/conf:ro
      - /usr/bin/hdfs:/usr/bin/hdfs:ro
      - /usr/bin/hadoop:/usr/bin/hadoop:ro
      - /usr/bin/spark-shell:/usr/bin/spark-shell:ro
      - /usr/bin/pyspark:/usr/bin/pyspark:ro
      - /usr/lib/bigtop-utils:/usr/lib/bigtop-utils:ro
      - /usr/lib/hadoop:/usr/lib/hadoop:ro
      - /usr/lib/hadoop-hdfs:/usr/lib/hadoop-hdfs:ro
      - /usr/lib/hadoop-kms:/usr/lib/hadoop-kms:ro
      - /usr/lib/hadoop-httpfs:/usr/lib/hadoop-httpfs:ro
      - /usr/lib/hadoop-lzo:/usr/lib/hadoop-lzo:ro
      - /usr/lib/hadoop-yarn:/usr/lib/hadoop-yarn:ro
      - /usr/lib/hadoop-mapreduce:/usr/lib/hadoop-mapreduce:ro
      - /usr/lib/spark:/usr/lib/spark:ro
      - /usr/share/aws:/usr/share/aws:ro
      - /mnt/s3:/mnt/s3:rw
      - /mnt/var/lib/hadoop/tmp:/mnt/var/lib/hadoop/tmp:rw
      # IMPORTANT BIND MOUNTS FOR EMR ABOVE, OVERRIDE THEM IF NECESSARY OR ADD ADDITIONAL VOLUMES BELOW
  webserver:
    build:
      context: .
      dockerfile: Dockerfile-runnable
    command: ['afp-webserver']
    depends_on:
      - scheduler
    environment:
      AIRFLOW_EMAIL: fixme@data.gov.sg
      AIRFLOW_PASSWORD: fixme
      AIRFLOW_USER: fixme
      AIRFLOW__CORE__FERNET_KEY: ZZvP_7JEhKfQ7v5sg1DV1o4b1DcGv3Tu9vJXzEKkxbE=
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql://fixme:fixme@postgres:5432/airflow
    hostname: webserver
    ports:
      - 8888:8080
    restart: always
    volumes_from:
      - scheduler
volumes:
  airflow_logs: {}
